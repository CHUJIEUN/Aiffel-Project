{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f64b22bd",
   "metadata": {},
   "source": [
    "## Exploration4. 멋진 작사가 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8913c14f",
   "metadata": {},
   "source": [
    "### >데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ce08c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " [\"Now I've heard there was a secret chord\", 'That David played, and it pleased the Lord', \"But you don't really care for music, do you?\", 'It goes like this', 'The fourth, the fifth', 'The minor fall, the major lift', 'The baffled king composing Hallelujah Hallelujah', 'Hallelujah', 'Hallelujah', 'Hallelujah Your faith was strong but you needed proof']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os, re\n",
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/data_preprocess/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt 파일을 모두 읽어서 raw_corpus 에 담습니다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()  #텍스트를 라인 단위로 읽어오기\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c4d1f0",
   "metadata": {},
   "source": [
    "### > 데이터 다듬기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de52fa06",
   "metadata": {},
   "source": [
    "불러온 데이터에 괄호 등 불필요한 부분이 없으므로 공백인 문장 또는 중복문장 제외하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5eabfe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원래 데이터 크기 : 187088\n",
      "공백인 문장, 대괄호 포함된 문장 제외한 데이터 크기 : 174446\n",
      "중복되는 문장 제외한 데이터 크기 : 117114\n"
     ]
    }
   ],
   "source": [
    "corpus= []\n",
    "\n",
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue           # 길이가 0인 문장 제외\n",
    "    \n",
    "    corpus.append(sentence)\n",
    "\n",
    "\n",
    "print(f'원본 데이터 크기 : {len(raw_corpus)}')\n",
    "print(f'공백인 문장을 제외한 데이터 크기 : {len(corpus)}')\n",
    "\n",
    "corpus = list(set(corpus))          # 중복되는 문장 제외 후 다시 리스트로 변환\n",
    "print(f'중복되는 문장 제외한 데이터 크기 : {len(corpus)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790e8e47",
   "metadata": {},
   "source": [
    "### > 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66057e92",
   "metadata": {},
   "source": [
    "1. 소문자로 바꾸고, 양쪽 공백 삭제\n",
    "2. 특수문자 양쪽에 공백 삽입\n",
    "3. 여러개의 공백은 하나의 공백으로 바꿈\n",
    "4. a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿈\n",
    "5. 다시 양쪽 공백 삭제\n",
    "6. 문장 시작에는 <start>, 끝에는 <end>를 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0803763",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()                     # 1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence)     # 2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)             # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence)   # 4\n",
    "    sentence = sentence.strip()                             # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>'             # 6\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d496893b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> but treat dimes fair and im <end>',\n",
       " '<start> and i d take up for him <end>',\n",
       " '<start> i tell you what , it don t bother me nohow , <end>',\n",
       " '<start> where everything you want <end>',\n",
       " '<start> you give it one more chance <end>',\n",
       " '<start> i ma be around forever , entertain even in the ground <end>',\n",
       " '<start> i like my ice frozen like the antartic <end>',\n",
       " '<start> id rather spend a ching aling on it eh eh eh <end>',\n",
       " '<start> but i know what you are <end>',\n",
       " '<start> straight up how many times i gotta tell that ass to come over ? <end>']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_tokenized = []\n",
    "\n",
    "for sentence in corpus:\n",
    "    \n",
    "    \n",
    "    # 데이터 정제하기\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus_tokenized.append(preprocessed_sentence)\n",
    "        \n",
    "# 10개의 결과만 확인\n",
    "corpus_tokenized[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01e19f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2  31 850 ...   0   0   0]\n",
      " [  2   8   5 ...   0   0   0]\n",
      " [  2   5  91 ...   4   3   0]\n",
      " ...\n",
      " [  2   8   9 ... 370  54   3]\n",
      " [  2   8 399 ...   0   0   0]\n",
      " [  2  22  79 ...   0   0   0]] <keras_preprocessing.text.Tokenizer object at 0x7f7f7b830a00>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000,         # 12000개 단어를 기억할 수 있음\n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"        # 포함되지 않는 단어는 <unk> 으로 표현\n",
    "    )\n",
    "    \n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    \n",
    "\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=15)\n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus_tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906cd43c",
   "metadata": {},
   "source": [
    "### > 단어사전 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b021e22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e354e012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2   31  850 5169 1169    8  120    3    0    0    0    0    0    0]\n",
      "[  31  850 5169 1169    8  120    3    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1] # 소스문장 생성 \n",
    "\n",
    "tgt_input = tensor[:, 1:]  # 타켓 문장 생성\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce83d4cd",
   "metadata": {},
   "source": [
    "### > 데이터셋 분리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a46fc5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, \n",
    "                                                          tgt_input,\n",
    "                                                          test_size=0.2,\n",
    "                                                          shuffle=True, \n",
    "                                                          random_state=34) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f93f89b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (93691, 14)\n",
      "Target Train: (93691, 14)\n"
     ]
    }
   ],
   "source": [
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Target Train:\", dec_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a069ce2",
   "metadata": {},
   "source": [
    "### > 모델학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7550151f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "\n",
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super(TextGenerator, self).__init__()\n",
    "        \n",
    "        self.embedding = Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469b243f",
   "metadata": {},
   "source": [
    "단어 벡터 차원의 수인 embedding_size, LSTM레이어의 hidden_size 조절해주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49c94332",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 256\n",
    "hidden_size = 2048\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1822fdcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2928/2928 [==============================] - 307s 97ms/step - loss: 3.4321\n",
      "Epoch 2/10\n",
      "2928/2928 [==============================] - 304s 104ms/step - loss: 2.9925\n",
      "Epoch 3/10\n",
      "2928/2928 [==============================] - 311s 106ms/step - loss: 2.6803\n",
      "Epoch 4/10\n",
      "2928/2928 [==============================] - 313s 107ms/step - loss: 2.3282\n",
      "Epoch 5/10\n",
      "2928/2928 [==============================] - 311s 106ms/step - loss: 1.9747\n",
      "Epoch 6/10\n",
      "2928/2928 [==============================] - 311s 106ms/step - loss: 1.6656\n",
      "Epoch 7/10\n",
      "2928/2928 [==============================] - 310s 106ms/step - loss: 1.4200\n",
      "Epoch 8/10\n",
      "2928/2928 [==============================] - 311s 106ms/step - loss: 1.2542\n",
      "Epoch 9/10\n",
      "2928/2928 [==============================] - 310s 106ms/step - loss: 1.1535\n",
      "Epoch 10/10\n",
      "2928/2928 [==============================] - 311s 106ms/step - loss: 1.0944\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7f6c0955e0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer)\n",
    "model.fit(enc_train, dec_train, epochs=10) #epochs 값 10으로 지정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a73f45",
   "metadata": {},
   "source": [
    "epoch 10으로 학습결과 loss는 1.0944이다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19016658",
   "metadata": {},
   "source": [
    "### > 모델평가하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ee04be",
   "metadata": {},
   "source": [
    "1. 입력받은 문장의 텐서를 입력함\n",
    "2. 예측된 값 중 가장 높은 확률인 word index를 뽑아냄\n",
    "3. 2에서 예측된 word index를 문장 뒤에 붙인다.\n",
    "4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성 종료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "098eaa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_text 함수를 사용하여 작문 진행\n",
    "def generate_text(model, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 일단 텐서로 변환합니다.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    " \n",
    "    while True:\n",
    "        predict = model(test_tensor)  # 1\n",
    "        \n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]\n",
    "        \n",
    "        # 3\n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "\n",
    "        # 4\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "     # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다 \n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e8efcf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> you are the greatest thing to me . <end> '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> you are\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96b7357d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i always have respected her for busting out and gettin free <end> '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> i always\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e04495a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> if you re wondering , know that i m not sorry <end> '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> if\", max_len=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39247d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> there s no one in life quite like you , you baby <end> '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(model, tokenizer, init_sentence=\"<start> there\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d7ab19",
   "metadata": {},
   "source": [
    "#### > 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2233638e",
   "metadata": {},
   "source": [
    "중간중간에 텐서플로우를 선언했는데 자꾸 오류가 떠서 고생했다.\n",
    "결과물을 보면 생각보다 sweet한 문장을 만들어낸다. 모델이 감성적이다.\n",
    "대충 이해해서 그렇지만 조금 어색한 문장이 있을것 같기도 하다.\n",
    "노드 실습때는 할만하다고 생각했는데 몇일이 지나고 다시 보니\n",
    "까먹어서 다시 훑어보았다.. 몇일 차이로 프로젝트가 조금더 어렵게 느껴졌다.\n",
    "조금 더 다양한 전처리를 활용해보고 싶다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b21dc43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
